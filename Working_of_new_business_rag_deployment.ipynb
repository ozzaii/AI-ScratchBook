{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXsmF8xDF07E5e6TIqCqGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fd171ce51c446568dce7b5ffc4da5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fe57eee554a4716bba82522c8442d0f",
              "IPY_MODEL_9d4642086aa84bd2a005cdc04f91e157",
              "IPY_MODEL_bf0598743f9a4c588e11e96f0cfb08cc"
            ],
            "layout": "IPY_MODEL_aa0580bf03fa4f188ee931516bfb4184"
          }
        },
        "7fe57eee554a4716bba82522c8442d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac9219add01d4a4dbb7c2caad2f6cc75",
            "placeholder": "​",
            "style": "IPY_MODEL_c1756ae2ccc94566a893f79273a5a20a",
            "value": "Processing records: 100%"
          }
        },
        "9d4642086aa84bd2a005cdc04f91e157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dc57ec88f674886850eb392b99ea7c6",
            "max": 41398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f494c8f518a1467caccc4a758779df1b",
            "value": 41398
          }
        },
        "bf0598743f9a4c588e11e96f0cfb08cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d34f553eef2d40c59cc5fe3e7e331a09",
            "placeholder": "​",
            "style": "IPY_MODEL_ca182c1c055c43bd814241b084b44837",
            "value": " 41398/41398 [00:02&lt;00:00, 16771.66it/s]"
          }
        },
        "aa0580bf03fa4f188ee931516bfb4184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9219add01d4a4dbb7c2caad2f6cc75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1756ae2ccc94566a893f79273a5a20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dc57ec88f674886850eb392b99ea7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f494c8f518a1467caccc4a758779df1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d34f553eef2d40c59cc5fe3e7e331a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca182c1c055c43bd814241b084b44837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae690342c366410bb64fe9271fc2bc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f6ead4f6c254a89b08abe4eb63b0b45",
              "IPY_MODEL_08281d59f83e4634a879803080bdef37",
              "IPY_MODEL_dfe4ae8bcaab4cdfada2b7a322027f3f"
            ],
            "layout": "IPY_MODEL_91e0fc873c76468b80aad57666d1ef48"
          }
        },
        "4f6ead4f6c254a89b08abe4eb63b0b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b164a78317b44b0c8ad60d7bc65e2881",
            "placeholder": "​",
            "style": "IPY_MODEL_a1cc40d5f5304cf7914b9f6f258ce4d8",
            "value": "Creating text representations: 100%"
          }
        },
        "08281d59f83e4634a879803080bdef37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ae0e92be284eb491ef93f40871b476",
            "max": 41398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_372b5a2078d8430ba77f38ab3d9fca0e",
            "value": 41398
          }
        },
        "dfe4ae8bcaab4cdfada2b7a322027f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63abfdbd1977477eba4e52450062eb98",
            "placeholder": "​",
            "style": "IPY_MODEL_40d253b00e48431e9ccb832da9ee05b3",
            "value": " 41398/41398 [00:00&lt;00:00, 457531.10it/s]"
          }
        },
        "91e0fc873c76468b80aad57666d1ef48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b164a78317b44b0c8ad60d7bc65e2881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cc40d5f5304cf7914b9f6f258ce4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ae0e92be284eb491ef93f40871b476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372b5a2078d8430ba77f38ab3d9fca0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63abfdbd1977477eba4e52450062eb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d253b00e48431e9ccb832da9ee05b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0d24007ae9b4681830001a0ec7d461d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3c1670418ac47d8b397d867a40bb6d4",
              "IPY_MODEL_a64fbcd541c4428fb45ee7d860c73eae",
              "IPY_MODEL_5a0f25dd78104d79afaa8caec3a9efe7"
            ],
            "layout": "IPY_MODEL_7cbe734f74bb40b8b691cfce633fb742"
          }
        },
        "a3c1670418ac47d8b397d867a40bb6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4599a0cb45164ee9b167531799745db3",
            "placeholder": "​",
            "style": "IPY_MODEL_692348a1f4cf47a68d95cdd4f14b2b05",
            "value": "Generating embeddings: 100%"
          }
        },
        "a64fbcd541c4428fb45ee7d860c73eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29522d236974d078ef502055a7c664e",
            "max": 1294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a37c67a11804c418e0e37d42762d6c2",
            "value": 1294
          }
        },
        "5a0f25dd78104d79afaa8caec3a9efe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2f2240afbe4f2fb390ccc2820e14e2",
            "placeholder": "​",
            "style": "IPY_MODEL_56c9df4720c94e22a2811709563a46b9",
            "value": " 1294/1294 [03:57&lt;00:00,  5.90it/s]"
          }
        },
        "7cbe734f74bb40b8b691cfce633fb742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4599a0cb45164ee9b167531799745db3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692348a1f4cf47a68d95cdd4f14b2b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f29522d236974d078ef502055a7c664e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a37c67a11804c418e0e37d42762d6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d2f2240afbe4f2fb390ccc2820e14e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c9df4720c94e22a2811709563a46b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozzaii/AI-ScratchBook/blob/main/Working_of_new_business_rag_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIsRNX848ziZ",
        "outputId": "7e32d286-9b50-4d82-a47b-2d2b2f14bf82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2024.10.7-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Collecting pickle-mixin\n",
            "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Collecting unsloth-zoo (from unsloth)\n",
            "  Downloading unsloth_zoo-2024.10.4-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting datasets>=2.16.0 (from unsloth)\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.34.2)\n",
            "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Collecting hf-transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting langchain-core<0.4,>=0.3.0 (from langchain_huggingface)\n",
            "  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.4 (from langchain_community)\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.4->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (2.9.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading unsloth-2024.10.7-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.137-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2024.10.4-py3-none-any.whl (39 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=5990 sha256=85c909506f2a2a385da20fd7001e6d42642cfcda7c203d73492a1a6dd6183c66\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/c6/e9/d1b0a34e1efc6c3ec9c086623972c6de6317faddb2af0a619c\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin, xxhash, triton, shtab, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, hf-transfer, h11, faiss-cpu, dill, typing-inspect, requests-toolbelt, multiprocess, jsonpatch, httpcore, xformers, tyro, pydantic-settings, httpx, dataclasses-json, bitsandbytes, langsmith, sentence-transformers, peft, langchain-core, datasets, trl, langchain-text-splitters, langchain_huggingface, unsloth-zoo, langchain, unsloth, langchain_community\n",
            "Successfully installed bitsandbytes-0.44.1 dataclasses-json-0.6.7 datasets-3.0.2 dill-0.3.8 faiss-cpu-1.9.0 h11-0.14.0 hf-transfer-0.1.8 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.13 langchain-text-splitters-0.3.0 langchain_community-0.3.3 langchain_huggingface-0.1.0 langsmith-0.1.137 marshmallow-3.23.0 multiprocess-0.70.16 mypy-extensions-1.0.0 orjson-3.10.10 peft-0.13.2 pickle-mixin-1.0.2 pydantic-settings-2.6.0 python-dotenv-1.0.1 requests-toolbelt-1.0.0 sentence-transformers-3.2.1 shtab-1.7.1 triton-3.1.0 trl-0.11.1 typing-inspect-0.9.0 tyro-0.8.14 unsloth-2024.10.7 unsloth-zoo-2024.10.4 xformers-0.0.28.post2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers unsloth langchain_huggingface langchain_community faiss-cpu pandas tqdm pickle-mixin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly seaborn faiss-gpu"
      ],
      "metadata": {
        "id": "3ekJ-aflKNsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f897465a-f6f1-40dc-ec20-c2050d3497a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "import os\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple, Set\n",
        "import pandas as pd\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import calendar\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import spacy\n",
        "from dateutil.parser import parse\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from unsloth import FastLanguageModel\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Define File Paths and Constants\n",
        "BASE_PATH = \"/content/drive2/MyDrive/trade_analysis_model\"\n",
        "CHECKPOINT_PATH = f\"{BASE_PATH}/checkpoint-45\"\n",
        "CSV_PATH = f\"{BASE_PATH}/export_data.csv\"\n",
        "CHART_STORE_PATH = f\"{BASE_PATH}/charts\"\n",
        "INDEX_PATH = f\"{BASE_PATH}/rag_indices\"\n",
        "\n",
        "# Model Constants\n",
        "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
        "MAX_LENGTH = 2048\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "# Define Product Keywords for IntelligentRAG\n",
        "PRODUCT_KEYWORDS = {\n",
        "    'rice': ['rice', 'basmati', 'irri', 'sella', 'paddy', 'chawal', 'برن', 'چاول'],\n",
        "    'cotton': ['cotton', 'yarn', 'textile', 'fabric', 'کپاس', 'روئی'],\n",
        "    'wheat': ['wheat', 'flour', 'grain'],\n",
        "    'sugar': ['sugar', 'raw sugar', 'refined sugar', 'brown sugar']\n",
        "    # Add more product categories and their keywords as needed\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class TradeRecord:\n",
        "    date: datetime\n",
        "    description: str\n",
        "    origin: str\n",
        "    exporter: str\n",
        "    importer: str\n",
        "    quantity: float\n",
        "    value_pkr: float\n",
        "    unit: str\n",
        "    original_text: str\n",
        "\n",
        "class IntelligentRAG:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = base_path\n",
        "        self.index_path = f\"{base_path}/rag_indices\"\n",
        "        self.semantic_index_path = f\"{self.index_path}/semantic_index\"\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        os.makedirs(self.semantic_index_path, exist_ok=True)\n",
        "\n",
        "        # Initialize sentence transformer\n",
        "        self.embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "        self.embed_dim = self.embed_model.get_sentence_embedding_dimension()\n",
        "\n",
        "        # Initialize indices\n",
        "        self.faiss_index = None\n",
        "        self.record_embeddings = None\n",
        "        self.records = []\n",
        "\n",
        "    def _build_semantic_index(self, force_rebuild: bool = False):\n",
        "        \"\"\"Build and save semantic search index\"\"\"\n",
        "        index_files = {\n",
        "            'faiss': f\"{self.semantic_index_path}/faiss.index\",\n",
        "            'embeddings': f\"{self.semantic_index_path}/embeddings.npy\",\n",
        "            'records': f\"{self.semantic_index_path}/records.pkl\"\n",
        "        }\n",
        "\n",
        "        # Check if index exists\n",
        "        if not force_rebuild and all(os.path.exists(f) for f in index_files.values()):\n",
        "            print(\"Loading existing semantic index...\")\n",
        "            try:\n",
        "                # Load FAISS index\n",
        "                self.faiss_index = faiss.read_index(index_files['faiss'])\n",
        "                # Load embeddings\n",
        "                self.record_embeddings = np.load(index_files['embeddings'])\n",
        "                # Load records mapping\n",
        "                with open(index_files['records'], 'rb') as f:\n",
        "                    self.records = pickle.load(f)\n",
        "                print(f\"Loaded semantic index with {len(self.records)} records\")\n",
        "                return\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading semantic index: {e}\")\n",
        "                print(\"Rebuilding index...\")\n",
        "\n",
        "        print(\"Building new semantic index...\")\n",
        "        texts = []\n",
        "\n",
        "        # Create rich text representations\n",
        "        for record in tqdm(self.records, desc=\"Creating text representations\"):\n",
        "            text = f\"\"\"\n",
        "            Product: {record['description']}\n",
        "            Origin: {record['origin']}\n",
        "            Exporter: {record['exporter']}\n",
        "            Importer: {record['importer']}\n",
        "            Value: {record['value_pkr']} PKR\n",
        "            Quantity: {record['quantity']} {record['unit']}\n",
        "            \"\"\"\n",
        "            texts.append(text)\n",
        "\n",
        "        # Generate embeddings in batches\n",
        "        print(\"Generating embeddings...\")\n",
        "        batch_size = 32\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.embed_model.encode(batch, convert_to_tensor=True)\n",
        "                all_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "            # Clear CUDA cache periodically\n",
        "            if torch.cuda.is_available() and i % (batch_size * 10) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        self.record_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "        # Initialize and build FAISS index\n",
        "        print(\"Building FAISS index...\")\n",
        "        self.faiss_index = faiss.IndexFlatL2(self.embed_dim)\n",
        "        self.faiss_index.add(self.record_embeddings)\n",
        "\n",
        "        # Save all components\n",
        "        print(\"Saving semantic index...\")\n",
        "        try:\n",
        "            # Save FAISS index\n",
        "            faiss.write_index(self.faiss_index, index_files['faiss'])\n",
        "            # Save embeddings\n",
        "            np.save(index_files['embeddings'], self.record_embeddings)\n",
        "            # Save records mapping\n",
        "            with open(index_files['records'], 'wb') as f:\n",
        "                pickle.dump(self.records, f)\n",
        "            print(f\"Successfully saved semantic index with {len(self.records)} records\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving semantic index: {e}\")\n",
        "\n",
        "    def retrieve(self, query: str, time_info: Dict = None, country: str = None,\n",
        "                product_type: str = None, k: int = 20) -> List[Dict]:\n",
        "        \"\"\"Semantic search with filtering\"\"\"\n",
        "        try:\n",
        "            # Get query embedding\n",
        "            query_embedding = self.embed_model.encode([query])\n",
        "\n",
        "            # Perform semantic search\n",
        "            D, I = self.faiss_index.search(query_embedding, k * 2)  # Get more candidates for filtering\n",
        "\n",
        "            # Get candidate records\n",
        "            candidates = [self.records[i] for i in I[0]]\n",
        "\n",
        "            # Apply filters\n",
        "            filtered_records = []\n",
        "            for record in candidates:\n",
        "                # Time filter\n",
        "                if time_info and time_info.get('time_range'):\n",
        "                    start_date, end_date = time_info['time_range']\n",
        "                    if not (start_date <= record['date'] <= end_date):\n",
        "                        continue\n",
        "\n",
        "                # Country filter\n",
        "                if country and record['origin'].lower() != country.lower():\n",
        "                    continue\n",
        "\n",
        "                # Product filter\n",
        "                if product_type:\n",
        "                    product_keywords = PRODUCT_KEYWORDS.get(product_type, [])\n",
        "                    if not any(keyword in record['description'].lower() for keyword in product_keywords):\n",
        "                        continue\n",
        "\n",
        "                filtered_records.append(record)\n",
        "                if len(filtered_records) >= k:\n",
        "                    break\n",
        "\n",
        "            print(f\"Found {len(filtered_records)} relevant records after filtering\")\n",
        "            return filtered_records\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "    def load_data(self, csv_path: str, force_rebuild: bool = False):\n",
        "        \"\"\"Load data and build semantic search index\"\"\"\n",
        "        print(\"Loading trade data...\")\n",
        "        self.df = pd.read_csv(csv_path, low_memory=False)\n",
        "\n",
        "        # Convert dates and clean data\n",
        "        self.df['DATE'] = pd.to_datetime(self.df['DATE'], format='mixed', errors='coerce')\n",
        "        self.df = self.df.dropna(subset=['DATE'])\n",
        "\n",
        "        # Convert records to list of dicts for RAG\n",
        "        self.records = []\n",
        "        for _, row in tqdm(self.df.iterrows(), desc=\"Processing records\", total=len(self.df)):\n",
        "            record = {\n",
        "                'date': row['DATE'],\n",
        "                'description': str(row['DESCRIPTION']),\n",
        "                'origin': str(row['ORIGIN']),\n",
        "                'exporter': str(row['EXPORTER']),\n",
        "                'importer': str(row['IMPORTER']),\n",
        "                'quantity': float(row['QTY']),\n",
        "                'value_pkr': float(row['VALUE (PKR)']),\n",
        "                'unit': str(row['UNIT'])\n",
        "            }\n",
        "            self.records.append(record)\n",
        "\n",
        "        print(f\"Loaded {len(self.records)} records\")\n",
        "\n",
        "        # Build or load semantic search index\n",
        "        self._build_semantic_index(force_rebuild=force_rebuild)\n",
        "\n",
        "    def _create_record_text(self, row: pd.Series) -> str:\n",
        "        \"\"\"Create rich text representation for embedding\"\"\"\n",
        "        return f\"\"\"\n",
        "        Trade Record:\n",
        "        Product: {row['DESCRIPTION']}\n",
        "        Origin Country: {row['ORIGIN']}\n",
        "        Date: {row['DATE']}\n",
        "        Quantity: {row['QTY']} {row['UNIT']}\n",
        "        Value: {row['VALUE (PKR)']} PKR\n",
        "        Exporter: {row['EXPORTER']}\n",
        "        Importer: {row['IMPORTER']}\n",
        "        \"\"\"\n",
        "\n",
        "    def _update_indices(self, record: Dict, idx: int):\n",
        "        \"\"\"Update all index structures\"\"\"\n",
        "        # Time index\n",
        "        year = record['date'].year\n",
        "        month = record['date'].month\n",
        "        if year not in self.time_index:\n",
        "            self.time_index[year] = {}\n",
        "        if month not in self.time_index[year]:\n",
        "            self.time_index[year][month] = []\n",
        "        self.time_index[year][month].append(idx)\n",
        "\n",
        "        # Country index\n",
        "        country = record['origin'].lower()\n",
        "        if country not in self.country_index:\n",
        "            self.country_index[country] = []\n",
        "        self.country_index[country].append(idx)\n",
        "\n",
        "        # Product index\n",
        "        desc = record['description'].lower()\n",
        "        for product_type, keywords in PRODUCT_KEYWORDS.items():\n",
        "            if any(keyword in desc for keyword in keywords):\n",
        "                if product_type not in self.product_index:\n",
        "                    self.product_index[product_type] = []\n",
        "                self.product_index[product_type].append(idx)\n",
        "\n",
        "    def format_results(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Format retrieval results for analysis\"\"\"\n",
        "        if not results:\n",
        "            print(\"No results to format\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Compute aggregations\n",
        "            total_value = sum(record['value_pkr'] for record in results)\n",
        "            total_quantity = sum(record['quantity'] for record in results)\n",
        "\n",
        "            # Group by origin\n",
        "            origin_values = {}\n",
        "            for record in results:\n",
        "                origin = record['origin']\n",
        "                value = record['value_pkr']\n",
        "                origin_values[origin] = origin_values.get(origin, 0) + value\n",
        "\n",
        "            # Sort origins by value\n",
        "            top_origins = sorted(origin_values.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Group by product description\n",
        "            product_values = {}\n",
        "            for record in results:\n",
        "                desc = record['description']\n",
        "                value = record['value_pkr']\n",
        "                product_values[desc] = product_values.get(desc, 0) + value\n",
        "\n",
        "            # Sort products by value\n",
        "            top_products = sorted(product_values.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Format the results for analysis\n",
        "            formatted = {\n",
        "                'records': [{\n",
        "                    'date': record['date'],\n",
        "                    'description': record['description'],\n",
        "                    'origin': record['origin'],\n",
        "                    'exporter': record['exporter'],\n",
        "                    'importer': record['importer'],\n",
        "                    'quantity': record['quantity'],\n",
        "                    'value_pkr': record['value_pkr'],\n",
        "                    'unit': record['unit']\n",
        "                } for record in results],\n",
        "                'summary': {\n",
        "                    'total_records': len(results),\n",
        "                    'total_value_pkr': total_value,\n",
        "                    'total_quantity': total_quantity,\n",
        "                    'date_range': f\"{min(r['date'] for r in results)} to {max(r['date'] for r in results)}\",\n",
        "                    'top_origins': top_origins[:5],\n",
        "                    'top_products': top_products[:5]\n",
        "                },\n",
        "                'analysis': {\n",
        "                    'average_value': total_value / len(results) if results else 0,\n",
        "                    'origin_distribution': {\n",
        "                        origin: (value, (value / total_value * 100) if total_value else 0)\n",
        "                        for origin, value in origin_values.items()\n",
        "                    },\n",
        "                    'product_distribution': {\n",
        "                        desc: (value, (value / total_value * 100) if total_value else 0)\n",
        "                        for desc, value in product_values.items()\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Print summary for debugging\n",
        "            print(\"\\nResults Summary:\")\n",
        "            print(f\"Total Records: {formatted['summary']['total_records']}\")\n",
        "            print(f\"Total Value: {formatted['summary']['total_value_pkr']:,.2f} PKR\")\n",
        "            print(f\"Date Range: {formatted['summary']['date_range']}\")\n",
        "\n",
        "            print(\"\\nTop Origins by Value:\")\n",
        "            for origin, value in formatted['summary']['top_origins']:\n",
        "                percentage = (value / total_value * 100) if total_value else 0\n",
        "                print(f\"- {origin.capitalize()}: {value:,.2f} PKR ({percentage:.1f}%)\")\n",
        "\n",
        "            print(\"\\nTop Products by Value:\")\n",
        "            for product, value in formatted['summary']['top_products']:\n",
        "                percentage = (value / total_value * 100) if total_value else 0\n",
        "                print(f\"- {product.capitalize()}: {value:,.2f} PKR ({percentage:.1f}%)\")\n",
        "\n",
        "            return formatted\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error formatting results: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_query(self, query: str, time_info: Dict = None, country: str = None,\n",
        "                    product_type: str = None, k: int = 20) -> Dict:\n",
        "        \"\"\"Complete analysis pipeline\"\"\"\n",
        "        try:\n",
        "            # Retrieve relevant records\n",
        "            results = self.retrieve(query, time_info, country, product_type, k)\n",
        "\n",
        "            # Format results\n",
        "            formatted = self.format_results(results)\n",
        "\n",
        "            if not formatted:\n",
        "                return None\n",
        "\n",
        "            # Add query analysis\n",
        "            formatted['query_info'] = {\n",
        "                'original_query': query,\n",
        "                'time_info': time_info,\n",
        "                'country': country,\n",
        "                'product_type': product_type\n",
        "            }\n",
        "\n",
        "            return formatted\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analysis pipeline: {e}\")\n",
        "            return None\n",
        "\n",
        "class QueryParser:\n",
        "    \"\"\"Advanced query understanding and parsing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        import spacy\n",
        "        # Uncomment the following lines to enable GPU for spaCy\n",
        "        # spacy.prefer_gpu()\n",
        "\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        # Common trade-related terms\n",
        "        self.product_terms = {\n",
        "            'rice': ['basmati', 'white rice', 'brown rice', 'long grain', 'broken rice'],\n",
        "            'cotton': ['raw cotton', 'cotton yarn', 'cotton fabric'],\n",
        "            'wheat': ['wheat flour', 'wheat grain'],\n",
        "            'sugar': ['raw sugar', 'refined sugar', 'brown sugar']\n",
        "        }\n",
        "        self.time_terms = {\n",
        "            'months': list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:],\n",
        "            'quarters': ['q1', 'q2', 'q3', 'q4', 'quarter 1', 'quarter 2', 'quarter 3', 'quarter 4'],\n",
        "            'periods': ['year', 'month', 'week', 'day']\n",
        "        }\n",
        "\n",
        "        # Time-related patterns\n",
        "        self.month_map = {month.lower(): num for num, month in enumerate(calendar.month_name) if month}\n",
        "        self.quarter_map = {\n",
        "            'q1': [1, 2, 3],\n",
        "            'q2': [4, 5, 6],\n",
        "            'q3': [7, 8, 9],\n",
        "            'q4': [10, 11, 12]\n",
        "        }\n",
        "\n",
        "        # Product categories and their variations\n",
        "        self.product_patterns = {\n",
        "            'rice': {\n",
        "                'keywords': ['rice', 'basmati', 'irri', 'sella', 'paddy', 'chawal', 'برنج', 'چاول'],\n",
        "                'variants': {\n",
        "                    'basmati': ['super basmati', 'basmati sella', 'brown basmati', 'premium basmati'],\n",
        "                    'irri': ['irri-6', 'irri6', 'irri 6', 'irri-9', 'irri9', 'irri 9'],\n",
        "                    'broken': ['broken rice', 'rice broken', '100% broken', 'double broken'],\n",
        "                    'white': ['white rice', 'milled rice', 'polished rice']\n",
        "                }\n",
        "            },\n",
        "            'cotton': {\n",
        "                'keywords': ['cotton', 'yarn', 'textile', 'fabric', 'کپاس', 'روئی'],\n",
        "                'variants': {\n",
        "                    'raw': ['raw cotton', 'seed cotton', 'cotton lint'],\n",
        "                    'processed': ['cotton yarn', 'cotton fabric', 'cotton textile']\n",
        "                }\n",
        "            }\n",
        "            # Add other product categories as needed\n",
        "        }\n",
        "\n",
        "        # Trade action patterns\n",
        "        self.trade_patterns = {\n",
        "            'import': [\n",
        "                r'import[s|ed|ing]?',\n",
        "                r'bring[s|ing]? in',\n",
        "                r'bought from',\n",
        "                r'purchased from',\n",
        "                r'receiving from',\n",
        "                r'incoming',\n",
        "                r'coming in'\n",
        "            ],\n",
        "            'export': [\n",
        "                r'export[s|ed|ing]?',\n",
        "                r'send[s|ing]? to',\n",
        "                r'sold to',\n",
        "                r'shipping to',\n",
        "                r'outgoing',\n",
        "                r'going to'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Time expressions\n",
        "        self.time_patterns = {\n",
        "            'explicit_date': [\n",
        "                r'(?P<month>january|february|march|april|may|june|july|august|september|october|november|december)\\s+(?P<year>20\\d{2})',\n",
        "                r'(?P<quarter>q[1-4])\\s+(?P<year>20\\d{2})',\n",
        "                r'(?P<year>20\\d{2})\\s+(?P<quarter>q[1-4])',\n",
        "            ],\n",
        "            'relative_date': [\n",
        "                r'last (?P<unit>month|quarter|year)',\n",
        "                r'past (?P<number>\\d+) (?P<unit>month|quarter|year)s?',\n",
        "                r'previous (?P<unit>month|quarter|year)',\n",
        "                r'current (?P<unit>month|quarter|year)',\n",
        "                r'this (?P<unit>month|quarter|year)'\n",
        "            ],\n",
        "            'year_only': r'\\b20\\d{2}\\b',\n",
        "            'month_only': '|'.join(calendar.month_name[1:] + [m[:3] for m in calendar.month_name[1:]])\n",
        "        }\n",
        "\n",
        "    def _extract_time_info(self, query: str) -> Dict:\n",
        "        \"\"\"Parse query into structured components\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        time_info = {\n",
        "            'year': None,\n",
        "            'month': None,\n",
        "            'quarter': None,\n",
        "            'time_range': None,\n",
        "            'is_relative': False\n",
        "        }\n",
        "\n",
        "        # Match quarters (e.g., \"q1 2024\", \"Q2\")\n",
        "        quarter_pattern = r'q([1-4])(?:\\s*20)?(\\d{2,4})?'\n",
        "        quarter_match = re.search(quarter_pattern, query_lower)\n",
        "\n",
        "        # Match months (e.g., \"april 2024\", \"jan\", \"december 23\")\n",
        "        month_pattern = r'(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\s*(?:20)?(\\d{2,4})?'\n",
        "        month_match = re.search(month_pattern, query_lower)\n",
        "\n",
        "        if quarter_match:\n",
        "            quarter = int(quarter_match.group(1))\n",
        "            year = quarter_match.group(2)\n",
        "\n",
        "            if year:\n",
        "                year = int('20' + year if len(year) == 2 else year)\n",
        "            else:\n",
        "                year = datetime.now().year\n",
        "\n",
        "            time_info['year'] = year\n",
        "            time_info['quarter'] = quarter\n",
        "\n",
        "            start_month = 3 * quarter - 2\n",
        "            end_month = 3 * quarter\n",
        "            time_info['time_range'] = (\n",
        "                datetime(year, start_month, 1),\n",
        "                datetime(year, end_month, calendar.monthrange(year, end_month)[1])\n",
        "            )\n",
        "\n",
        "        elif month_match:\n",
        "            month_str = month_match.group(1)\n",
        "            year = month_match.group(2)\n",
        "\n",
        "            # Convert month string to number\n",
        "            month_map = {\n",
        "                'jan': 1, 'january': 1,\n",
        "                'feb': 2, 'february': 2,\n",
        "                'mar': 3, 'march': 3,\n",
        "                'apr': 4, 'april': 4,\n",
        "                'may': 5,\n",
        "                'jun': 6, 'june': 6,\n",
        "                'jul': 7, 'july': 7,\n",
        "                'aug': 8, 'august': 8,\n",
        "                'sep': 9, 'september': 9,\n",
        "                'oct': 10, 'october': 10,\n",
        "                'nov': 11, 'november': 11,\n",
        "                'dec': 12, 'december': 12\n",
        "            }\n",
        "\n",
        "            month = month_map[month_str[:3]]\n",
        "\n",
        "            if year:\n",
        "                year = int('20' + year if len(year) == 2 else year)\n",
        "            else:\n",
        "                year = datetime.now().year\n",
        "\n",
        "            time_info['year'] = year\n",
        "            time_info['month'] = month\n",
        "            time_info['time_range'] = (\n",
        "                datetime(year, month, 1),\n",
        "                datetime(year, month, calendar.monthrange(year, month)[1])\n",
        "            )\n",
        "\n",
        "        # Rest of the parsing code remains the same...\n",
        "\n",
        "        return time_info\n",
        "\n",
        "    def _extract_trade_type(self, query: str) -> str:\n",
        "        \"\"\"Intelligently determine trade direction from query\"\"\"\n",
        "        query = query.lower()\n",
        "\n",
        "        # Check explicit patterns\n",
        "        for trade_type, patterns in self.trade_patterns.items():\n",
        "            if any(re.search(pattern, query) for pattern in patterns):\n",
        "                return trade_type\n",
        "\n",
        "        # Context-based inference\n",
        "        doc = self.nlp(query)\n",
        "\n",
        "        # Look for directional prepositions and their objects\n",
        "        for token in doc:\n",
        "            if token.dep_ == 'prep' and token.text in ['to', 'from']:\n",
        "                if token.text == 'from':\n",
        "                    return 'import'\n",
        "                else:\n",
        "                    return 'export'\n",
        "\n",
        "        # Default to 'export' if direction is ambiguous\n",
        "        return 'export'\n",
        "\n",
        "    def _extract_product_info(self, query: str) -> Dict:\n",
        "        \"\"\"Extract detailed product information with variants\"\"\"\n",
        "        query = query.lower()\n",
        "        product_info = {\n",
        "            'main_category': None,\n",
        "            'subcategory': None,\n",
        "            'variants': set()\n",
        "        }\n",
        "\n",
        "        # Check each product category\n",
        "        for category, patterns in self.product_patterns.items():\n",
        "            # Check main keywords\n",
        "            if any(keyword in query for keyword in patterns['keywords']):\n",
        "                product_info['main_category'] = category\n",
        "\n",
        "                # Check variants\n",
        "                for subcategory, variant_patterns in patterns['variants'].items():\n",
        "                    if any(variant in query for variant in variant_patterns):\n",
        "                        product_info['subcategory'] = subcategory\n",
        "                        product_info['variants'].add(next(variant for variant in variant_patterns if variant in query))\n",
        "\n",
        "                break\n",
        "\n",
        "        return product_info\n",
        "\n",
        "    def _extract_location_info(self, query: str) -> Dict:\n",
        "        \"\"\"Extract location information with context\"\"\"\n",
        "        doc = self.nlp(query)\n",
        "        location_info = {\n",
        "            'countries': set(),\n",
        "            'regions': set(),\n",
        "            'context': None  # 'source', 'destination', or None\n",
        "        }\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['GPE', 'LOC']:\n",
        "                # Try to determine if it's a source or destination\n",
        "                context = None\n",
        "                for token in ent.subtree:\n",
        "                    if token.text in ['from', 'in']:\n",
        "                        context = 'source'\n",
        "                    elif token.text in ['to', 'for']:\n",
        "                        context = 'destination'\n",
        "\n",
        "                if context:\n",
        "                    location_info['context'] = context\n",
        "\n",
        "                if ent.label_ == 'GPE':\n",
        "                    location_info['countries'].add(ent.text.lower())\n",
        "                else:\n",
        "                    location_info['regions'].add(ent.text.lower())\n",
        "\n",
        "        return location_info\n",
        "\n",
        "    def parse_query(self, query: str) -> Dict:\n",
        "        \"\"\"Main parsing function that combines all extractors\"\"\"\n",
        "        parsed = {\n",
        "            'time_info': self._extract_time_info(query),\n",
        "            'trade_type': self._extract_trade_type(query),\n",
        "            'product_info': self._extract_product_info(query),\n",
        "            'location_info': self._extract_location_info(query),\n",
        "            'original_query': query\n",
        "        }\n",
        "\n",
        "        # Add confidence scores\n",
        "        parsed['confidence'] = self._calculate_confidence(parsed)\n",
        "\n",
        "        # Add query interpretation\n",
        "        parsed['interpretation'] = self._generate_interpretation(parsed)\n",
        "\n",
        "        return parsed\n",
        "\n",
        "    def _calculate_confidence(self, parsed: Dict) -> Dict:\n",
        "        \"\"\"Calculate confidence scores for each component\"\"\"\n",
        "        confidence = {\n",
        "            'time': 0.0,\n",
        "            'trade_type': 0.0,\n",
        "            'product': 0.0,\n",
        "            'location': 0.0,\n",
        "            'overall': 0.0\n",
        "        }\n",
        "\n",
        "        # Time confidence\n",
        "        if parsed['time_info']['time_range']:\n",
        "            confidence['time'] = 1.0\n",
        "        elif parsed['time_info']['year']:\n",
        "            confidence['time'] = 0.7\n",
        "\n",
        "        # Trade type confidence\n",
        "        if parsed['trade_type']:\n",
        "            confidence['trade_type'] = 0.8\n",
        "\n",
        "        # Product confidence\n",
        "        if parsed['product_info']['main_category']:\n",
        "            confidence['product'] = 0.7\n",
        "            if parsed['product_info']['subcategory']:\n",
        "                confidence['product'] = 0.9\n",
        "\n",
        "        # Location confidence\n",
        "        if parsed['location_info']['countries']:\n",
        "            confidence['location'] = 0.9\n",
        "        elif parsed['location_info']['regions']:\n",
        "            confidence['location'] = 0.7\n",
        "\n",
        "        # Calculate overall confidence\n",
        "        weights = {'time': 0.3, 'trade_type': 0.2, 'product': 0.25, 'location': 0.25}\n",
        "        confidence['overall'] = sum(confidence[k] * weights[k] for k in weights)\n",
        "\n",
        "        return confidence\n",
        "\n",
        "    def _generate_interpretation(self, parsed: Dict) -> str:\n",
        "        \"\"\"Generate human-readable interpretation of the query\"\"\"\n",
        "        parts = []\n",
        "\n",
        "        # Add trade type\n",
        "        if parsed['trade_type']:\n",
        "            parts.append(f\"Looking for {parsed['trade_type']}s\")\n",
        "\n",
        "        # Add product info\n",
        "        if parsed['product_info']['main_category']:\n",
        "            product_desc = parsed['product_info']['main_category']\n",
        "            if parsed['product_info']['subcategory']:\n",
        "                product_desc += f\" ({parsed['product_info']['subcategory']})\"\n",
        "            parts.append(f\"of {product_desc}\")\n",
        "\n",
        "        # Add location info\n",
        "        if parsed['location_info']['countries']:\n",
        "            countries = ', '.join(parsed['location_info']['countries'])\n",
        "            context = f\"{parsed['location_info']['context']} \" if parsed['location_info']['context'] else \"\"\n",
        "            parts.append(f\"{context}{countries}\")\n",
        "\n",
        "        # Add time info\n",
        "        if parsed['time_info']['time_range']:\n",
        "            start, end = parsed['time_info']['time_range']\n",
        "            parts.append(f\"during {start.strftime('%B %Y')} to {end.strftime('%B %Y')}\")\n",
        "\n",
        "        return ' '.join(parts).capitalize()\n",
        "\n",
        "class EnhancedTradeAnalysisSystem:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Enhanced Trade Analysis System...\")\n",
        "        self._setup_logging()\n",
        "        self.model_context_length = MAX_LENGTH\n",
        "        self.max_input_length = 1500\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for path in [BASE_PATH, CHART_STORE_PATH]:\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "\n",
        "        # Initialize components\n",
        "        self.query_parser = QueryParser()\n",
        "        self.model, self.tokenizer = self._init_model()\n",
        "\n",
        "        # Initialize IntelligentRAG\n",
        "        self.rag = IntelligentRAG(BASE_PATH)\n",
        "\n",
        "        try:\n",
        "            # Load data and build indices\n",
        "            self.rag.load_data(CSV_PATH)\n",
        "            print(\"Successfully loaded all indices!\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Enhanced system prompt\n",
        "        self.system_prompt = \"\"\"You are an expert trade data analyst. Analyze the provided trade data and focus on:\n",
        "1. Import/Export patterns and trends\n",
        "2. Key trading partners and their significance\n",
        "3. Product-specific insights and market dynamics\n",
        "4. Time-based trends and seasonality\n",
        "5. Value chain analysis and pricing patterns\n",
        "\n",
        "Provide analysis in clear, structured format with:\n",
        "- Key findings and insights\n",
        "- Specific calculations and metrics\n",
        "- Market implications and recommendations\n",
        "- Notable patterns or anomalies\n",
        "\n",
        "Base all analysis strictly on the provided data. If data is insufficient, clearly state limitations.\"\"\"\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Initialize logging configuration\"\"\"\n",
        "        log_filename = f'{BASE_PATH}/trade_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.StreamHandler(),\n",
        "                logging.FileHandler(log_filename)\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger('TradeAnalysis')\n",
        "\n",
        "    def _init_model(self):\n",
        "        \"\"\"Initialize the language model using checkpoint-45\"\"\"\n",
        "        self.logger.info(\"Initializing language model from checkpoint-45...\")\n",
        "        try:\n",
        "            # Load the model and tokenizer from the specified checkpoint\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name=CHECKPOINT_PATH,\n",
        "                max_seq_length=self.model_context_length,\n",
        "                load_in_4bit=True,  # Adjust based on your model's requirements\n",
        "                trust_remote_code=True  # Set to True if the model uses custom code\n",
        "            )\n",
        "\n",
        "            # Prepare the model for inference\n",
        "            model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "            self.logger.info(\"Language model initialized successfully from checkpoint-45.\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Model initialization error: {e}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def _create_document(self, row: pd.Series) -> Tuple[str, Dict]:\n",
        "        \"\"\"Create a document from a dataframe row\"\"\"\n",
        "        try:\n",
        "            text = f\"{row['DESCRIPTION']} from {row['ORIGIN']} on {row['DATE']}\"\n",
        "            metadata = {\n",
        "                'date': row['DATE'],\n",
        "                'description': row['DESCRIPTION'],\n",
        "                'origin': row['ORIGIN'],\n",
        "                'value': row['VALUE (PKR)']\n",
        "            }\n",
        "            return text, metadata\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error creating document from row: {e}\", exc_info=True)\n",
        "            return None, None\n",
        "\n",
        "    def analyze(self, query: str):\n",
        "        \"\"\"Analyze trade data using IntelligentRAG\"\"\"\n",
        "        try:\n",
        "            # Parse query\n",
        "            parsed_query = self.query_parser.parse_query(query)\n",
        "            print(\"\\nParsed Query Components:\")\n",
        "            print(f\"Time Info: {parsed_query.get('time_info')}\")\n",
        "            print(f\"Location Info: {parsed_query.get('location_info')}\")\n",
        "            print(f\"Product Info: {parsed_query.get('product_info')}\")\n",
        "\n",
        "            # Use IntelligentRAG for retrieval\n",
        "            retrieved_records = self.rag.retrieve(\n",
        "                query=query,\n",
        "                time_info=parsed_query['time_info'],\n",
        "                country=next(iter(parsed_query['location_info']['countries']), None),\n",
        "                product_type=parsed_query['product_info']['main_category']\n",
        "            )\n",
        "\n",
        "            if not retrieved_records:\n",
        "                print(\"No relevant data found for the query.\")\n",
        "                return None\n",
        "\n",
        "            # Format the retrieved results\n",
        "            formatted_results = self.rag.format_results(retrieved_records)\n",
        "\n",
        "            if not formatted_results:\n",
        "                return None\n",
        "\n",
        "            # Generate analysis prompt\n",
        "            analysis_prompt = self._create_analysis_prompt(query, formatted_results['summary'])\n",
        "\n",
        "            # Generate AI analysis\n",
        "            ai_analysis = self._generate_analysis(analysis_prompt)\n",
        "\n",
        "            # Combine everything into final result\n",
        "            result = {\n",
        "                'raw_data': formatted_results['records'],\n",
        "                'summary': formatted_results['summary'],\n",
        "                'ai_analysis': ai_analysis,\n",
        "                'charts': self.create_visualizations(pd.DataFrame(formatted_results['records']))\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during analysis: {e}\", exc_info=True)\n",
        "            print(f\"Error during analysis: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _create_analysis_prompt(self, query: str, summary: Dict) -> str:\n",
        "        \"\"\"Create analysis prompt using summary data in the training format\"\"\"\n",
        "        formatted_data = {\n",
        "            \"analysis_request\": query,\n",
        "            \"trade_data\": {\n",
        "                \"monetary_metrics\": {\n",
        "                    \"total_trade_value_pkr\": summary['total_value_pkr'],\n",
        "                    \"total_quantity\": summary['total_quantity']\n",
        "                },\n",
        "                \"time_period\": str(summary['date_range']),\n",
        "                \"product_analysis\": [\n",
        "                    {\"product\": p, \"value_pkr\": v, \"share\": (v/summary['total_value_pkr'])*100}\n",
        "                    for p, v in summary['top_products']\n",
        "                ],\n",
        "                \"geographical_distribution\": [\n",
        "                    {\"country\": o, \"value_pkr\": v, \"share\": (v/summary['total_value_pkr'])*100}\n",
        "                    for o, v in summary['top_origins']\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"<|im_start|>system\n",
        "You are a mathematical trade analysis expert. Your task is to analyze trade data and provide precise, data-driven insights. Focus solely on the provided data - do not make assumptions about data not shown. Use exact numbers and percentages from the data. Do not engage in follow-up discussions or ask questions.\n",
        "<|im_end|>\n",
        "<|im_start|>human\n",
        "Analyze the following trade data to answer this specific question: {query}\n",
        "\n",
        "The complete dataset for analysis:\n",
        "{json.dumps(formatted_data, indent=2)}\n",
        "\n",
        "Provide a single, comprehensive analysis based only on this data. Include relevant calculations and percentages to support your insights. Do not speculate beyond the provided information.\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _generate_analysis(self, prompt: str) -> str:\n",
        "        \"\"\"Generate analysis using the FastLanguageModel\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_input_length\n",
        "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            # Generate response using the model\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1000,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                num_beams=1,  # Set to 1 to avoid beam search\n",
        "                repetition_penalty=1.2,\n",
        "                no_repeat_ngram_size=3,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up response\n",
        "            response = self._clean_response(response)\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating analysis: {e}\", exc_info=True)\n",
        "            # Fallback response if generation fails\n",
        "            sample_records = self.df.head(5).to_dict(orient='records')\n",
        "            fallback = \"I apologize, but I encountered an error while generating the analysis. Here are some sample records:\\n\"\n",
        "            for record in sample_records:\n",
        "                date_str = record['DATE'].strftime('%Y-%m-%d') if isinstance(record['DATE'], pd.Timestamp) else record['DATE']\n",
        "                fallback += f\"- {date_str}: {record['DESCRIPTION']} from {record['ORIGIN']}\\n\"\n",
        "            return fallback\n",
        "\n",
        "    def _clean_response(self, response: str) -> str:\n",
        "        \"\"\"Clean and format the model's response\"\"\"\n",
        "        # Remove system/human prompts if present\n",
        "        if \"system\" in response.lower():\n",
        "            try:\n",
        "                response = response.split(\"assistant\")[-1].strip()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        response = re.sub(r'```json\\s*|\\s*```', '', response)\n",
        "\n",
        "        # Clean up whitespace\n",
        "        response = re.sub(r'\\s+', ' ', response).strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _prepare_analysis_data(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Prepare comprehensive analysis data from the DataFrame\"\"\"\n",
        "        try:\n",
        "            # Extract basic metrics\n",
        "            values = df['VALUE (PKR)'].astype(float).tolist()\n",
        "            dates = df['date'].dropna().tolist()\n",
        "            origins = df['origin'].astype(str).str.lower().tolist()\n",
        "            products = df['description'].astype(str).str.lower().tolist()\n",
        "\n",
        "            # Calculate aggregates\n",
        "            analysis_data = {\n",
        "                'total_records': len(df),\n",
        "                'total_value': sum(values),\n",
        "                'avg_value': sum(values) / len(values) if values else 0,\n",
        "                'date_range': f\"{min(dates).strftime('%Y-%m-%d') if not pd.isnull(min(dates)) else 'Unknown'} to {max(dates).strftime('%Y-%m-%d') if not pd.isnull(max(dates)) else 'Unknown'}\",\n",
        "                'top_origins': self._get_top_items(origins, values),\n",
        "                'top_products': self._get_top_items(products, values)\n",
        "            }\n",
        "\n",
        "            return analysis_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error preparing analysis data: {e}\", exc_info=True)\n",
        "            return self._get_default_analysis_data()\n",
        "\n",
        "    def _get_top_items(self, items: List[str], values: List[float], top_n: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get top items by value with proper aggregation\"\"\"\n",
        "        item_values = {}\n",
        "        for item, value in zip(items, values):\n",
        "            item_values[item] = item_values.get(item, 0) + value\n",
        "\n",
        "        return sorted(\n",
        "            item_values.items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )[:top_n]\n",
        "\n",
        "    def _get_default_analysis_data(self) -> Dict:\n",
        "        \"\"\"Return default analysis data structure\"\"\"\n",
        "        return {\n",
        "            'total_records': 0,\n",
        "            'total_value': 0,\n",
        "            'avg_value': 0,\n",
        "            'date_range': 'Unknown',\n",
        "            'top_origins': [],\n",
        "            'top_products': []\n",
        "        }\n",
        "\n",
        "    def create_visualizations(self, df: pd.DataFrame) -> Dict[str, str]:\n",
        "        \"\"\"Create visualizations with proper column handling\"\"\"\n",
        "        try:\n",
        "            charts = {}\n",
        "\n",
        "            if df.empty:\n",
        "                return charts\n",
        "\n",
        "            # Convert date column if exists (handle both 'DATE' and 'date')\n",
        "            date_col = 'date' if 'date' in df.columns else 'DATE' if 'DATE' in df.columns else None\n",
        "            if date_col:\n",
        "                df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "            else:\n",
        "                print(\"Warning: No date column found for time series visualization\")\n",
        "                return charts\n",
        "\n",
        "            # Time series chart\n",
        "            fig = go.Figure()\n",
        "            df_sorted = df.sort_values('date')\n",
        "\n",
        "            # Value over time\n",
        "            value_col = 'value_pkr' if 'value_pkr' in df.columns else 'VALUE (PKR)' if 'VALUE (PKR)' in df.columns else None\n",
        "            if value_col:\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=df_sorted['date'],\n",
        "                    y=df_sorted[value_col],\n",
        "                    mode='lines+markers',\n",
        "                    name='Trade Value'\n",
        "                ))\n",
        "\n",
        "                fig.update_layout(\n",
        "                    title='Trade Value Over Time',\n",
        "                    xaxis_title='Date',\n",
        "                    yaxis_title='Value (PKR)',\n",
        "                    template='plotly_dark'\n",
        "                )\n",
        "                charts['time_series'] = fig\n",
        "\n",
        "            # Save charts\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            for name, fig in charts.items():\n",
        "                try:\n",
        "                    fig.write_html(f\"{self.chart_store_path}/{name}_{timestamp}.html\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not save chart {name}: {e}\")\n",
        "\n",
        "            return charts\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error creating visualizations: {e}\", exc_info=True)\n",
        "            return {}\n",
        "\n",
        "    def filter_trade_data(self, parsed_query: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Filter the DataFrame based on parsed query components\"\"\"\n",
        "        df_filtered = self.df.copy()\n",
        "\n",
        "        # Print initial shape for debugging\n",
        "        print(f\"\\nInitial records: {len(df_filtered)}\")\n",
        "\n",
        "        # Filter by date range first\n",
        "        time_info = parsed_query.get('time_info', {})\n",
        "        time_range = time_info.get('time_range')\n",
        "        if time_range:\n",
        "            start_date, end_date = time_range\n",
        "            # Convert dates to pandas datetime\n",
        "            df_filtered['DATE'] = pd.to_datetime(df_filtered['DATE'])\n",
        "            mask = (df_filtered['DATE'] >= start_date) & (df_filtered['DATE'] <= end_date)\n",
        "            df_filtered = df_filtered[mask]\n",
        "            print(f\"After date filter: {len(df_filtered)} records\")\n",
        "\n",
        "        # Filter by location\n",
        "        location_info = parsed_query.get('location_info', {})\n",
        "        countries = list(location_info.get('countries', []))\n",
        "        if countries:\n",
        "            # Case-insensitive country matching\n",
        "            country_mask = df_filtered['ORIGIN'].str.lower().isin([c.lower() for c in countries])\n",
        "            df_filtered = df_filtered[country_mask]\n",
        "            print(f\"After location filter: {len(df_filtered)} records\")\n",
        "\n",
        "        # Filter by product\n",
        "        product_info = parsed_query.get('product_info', {})\n",
        "        if product_info.get('main_category') in PRODUCT_KEYWORDS:\n",
        "            product_type = product_info.get('main_category')\n",
        "            # Comprehensive product-related terms based on PRODUCT_KEYWORDS\n",
        "            product_terms = PRODUCT_KEYWORDS.get(product_type, [])\n",
        "            # Create pattern for matching any product term\n",
        "            pattern = '|'.join([re.escape(term) for term in product_terms])\n",
        "            product_mask = df_filtered['DESCRIPTION'].str.lower().str.contains(pattern, na=False)\n",
        "            df_filtered = df_filtered[product_mask]\n",
        "            print(f\"After product filter: {len(df_filtered)} records\")\n",
        "\n",
        "        # Sort by date and value\n",
        "        df_filtered = df_filtered.sort_values(['DATE', 'VALUE (PKR)'], ascending=[True, False])\n",
        "\n",
        "        # Print example records for debugging\n",
        "        if not df_filtered.empty:\n",
        "            print(\"\\nSample matches:\")\n",
        "            sample = df_filtered.head(3)\n",
        "            for _, row in sample.iterrows():\n",
        "                date_str = row['DATE'].strftime('%Y-%m-%d') if isinstance(row['DATE'], pd.Timestamp) else row['DATE']\n",
        "                print(f\"Date: {date_str}, Origin: {row['ORIGIN']}, Description: {row['DESCRIPTION']}\")\n",
        "\n",
        "        return df_filtered\n",
        "\n",
        "    def load_data(self, csv_path: str):\n",
        "        \"\"\"Load and preprocess trade data\"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Convert DATE column to datetime\n",
        "        self.df['DATE'] = pd.to_datetime(self.df['DATE'])\n",
        "\n",
        "        # Create a clean date column without time\n",
        "        self.df['date_clean'] = self.df['DATE'].dt.date\n",
        "\n",
        "        # Ensure DESCRIPTION is string type\n",
        "        self.df['DESCRIPTION'] = self.df['DESCRIPTION'].astype(str)\n",
        "\n",
        "    def filter_records(self, time_info: Dict, product_info: Dict, location_info: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Filter records based on query components\"\"\"\n",
        "        filtered_df = self.df.copy()\n",
        "\n",
        "        # Time filtering\n",
        "        if time_info.get('time_range'):\n",
        "            start_date, end_date = time_info['time_range']\n",
        "            filtered_df = filtered_df[\n",
        "                (filtered_df['DATE'] >= start_date) &\n",
        "                (filtered_df['DATE'] <= end_date)\n",
        "            ]\n",
        "\n",
        "        # Product filtering\n",
        "        if product_info.get('main_category'):\n",
        "            category_keywords = PRODUCT_KEYWORDS[product_info['main_category']]\n",
        "            product_mask = filtered_df['DESCRIPTION'].str.lower().apply(\n",
        "                lambda x: any(keyword in x.lower() for keyword in category_keywords)\n",
        "            )\n",
        "            filtered_df = filtered_df[product_mask]\n",
        "\n",
        "        # Location filtering\n",
        "        if location_info.get('countries'):\n",
        "            filtered_df = filtered_df[\n",
        "                filtered_df['ORIGIN'].str.lower().isin(\n",
        "                    [c.lower() for c in location_info['countries']]\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        return filtered_df\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Initialize system\n",
        "        system = EnhancedTradeAnalysisSystem()\n",
        "\n",
        "        print(\"\\nEnhanced Trade Analysis System Ready!\")\n",
        "        print(\"Enter your queries about trade data. The system will provide detailed analysis.\")\n",
        "        print(\"Type 'quit' to exit.\")\n",
        "\n",
        "        while True:\n",
        "            query = input(\"\\nEnter your analysis question: \").strip()\n",
        "\n",
        "            if query.lower() == 'quit':\n",
        "                print(\"Exiting Enhanced Trade Analysis System. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not query:\n",
        "                print(\"Please enter a valid query.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = system.analyze(query)\n",
        "\n",
        "                if result is None:\n",
        "                    continue\n",
        "\n",
        "                # Print retrieved records (limiting to first 5 for brevity)\n",
        "                print(\"\\nRetrieved Records:\")\n",
        "                print(\"=\"*80)\n",
        "\n",
        "                for i, record in enumerate(result['raw_data'][:5], 1):\n",
        "                    date_str = record['date'].strftime('%Y-%m-%d') if isinstance(record['date'], pd.Timestamp) else record['date']\n",
        "                    print(f\"\\nRecord {i}:\")\n",
        "                    print(f\"Date: {date_str}\")\n",
        "                    print(f\"Product: {record['description']}\")\n",
        "                    print(f\"Origin: {record['origin']}\")\n",
        "                    print(f\"Value: {float(record['value_pkr']):,.2f} PKR\")\n",
        "                    print(\"-\"*40)\n",
        "\n",
        "                # Print summary stats\n",
        "                metadata = result['summary']\n",
        "                print(\"\\nSummary Statistics:\")\n",
        "                print(\"=\"*80)\n",
        "                print(f\"Total Records: {metadata['total_records']}\")\n",
        "                print(f\"Total Value: {metadata['total_value_pkr']:,.2f} PKR\")\n",
        "                print(f\"Date Range: {metadata['date_range']}\")\n",
        "\n",
        "                print(\"\\nTop Origins by Value:\")\n",
        "                for origin, value in metadata['top_origins']:\n",
        "                    percentage = (value / metadata['total_value_pkr'] * 100) if metadata['total_value_pkr'] else 0\n",
        "                    print(f\"- {origin.capitalize()}: {value:,.2f} PKR ({percentage:.1f}%)\")\n",
        "\n",
        "                print(\"\\nTop Products by Value:\")\n",
        "                for product, value in metadata['top_products']:\n",
        "                    percentage = (value / metadata['total_value_pkr'] * 100) if metadata['total_value_pkr'] else 0\n",
        "                    print(f\"- {product.capitalize()}: {value:,.2f} PKR ({percentage:.1f}%)\")\n",
        "\n",
        "                # Print AI analysis\n",
        "                print(\"\\nAI Analysis:\")\n",
        "                print(\"=\"*80)\n",
        "                print(result['ai_analysis'])\n",
        "\n",
        "                # Print visualization info\n",
        "                if result.get('charts'):\n",
        "                    print(\"\\nVisualizations have been saved to:\", CHART_STORE_PATH)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during analysis: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"System initialization error: {str(e)}\")\n",
        "        logging.error(f\"System initialization error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "VwTTkWtm9GRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8fd171ce51c446568dce7b5ffc4da5ed",
            "7fe57eee554a4716bba82522c8442d0f",
            "9d4642086aa84bd2a005cdc04f91e157",
            "bf0598743f9a4c588e11e96f0cfb08cc",
            "aa0580bf03fa4f188ee931516bfb4184",
            "ac9219add01d4a4dbb7c2caad2f6cc75",
            "c1756ae2ccc94566a893f79273a5a20a",
            "6dc57ec88f674886850eb392b99ea7c6",
            "f494c8f518a1467caccc4a758779df1b",
            "d34f553eef2d40c59cc5fe3e7e331a09",
            "ca182c1c055c43bd814241b084b44837",
            "ae690342c366410bb64fe9271fc2bc74",
            "4f6ead4f6c254a89b08abe4eb63b0b45",
            "08281d59f83e4634a879803080bdef37",
            "dfe4ae8bcaab4cdfada2b7a322027f3f",
            "91e0fc873c76468b80aad57666d1ef48",
            "b164a78317b44b0c8ad60d7bc65e2881",
            "a1cc40d5f5304cf7914b9f6f258ce4d8",
            "33ae0e92be284eb491ef93f40871b476",
            "372b5a2078d8430ba77f38ab3d9fca0e",
            "63abfdbd1977477eba4e52450062eb98",
            "40d253b00e48431e9ccb832da9ee05b3",
            "d0d24007ae9b4681830001a0ec7d461d",
            "a3c1670418ac47d8b397d867a40bb6d4",
            "a64fbcd541c4428fb45ee7d860c73eae",
            "5a0f25dd78104d79afaa8caec3a9efe7",
            "7cbe734f74bb40b8b691cfce633fb742",
            "4599a0cb45164ee9b167531799745db3",
            "692348a1f4cf47a68d95cdd4f14b2b05",
            "f29522d236974d078ef502055a7c664e",
            "9a37c67a11804c418e0e37d42762d6c2",
            "4d2f2240afbe4f2fb390ccc2820e14e2",
            "56c9df4720c94e22a2811709563a46b9"
          ]
        },
        "outputId": "56e4983e-817f-4cf9-fae5-faf307550fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Initializing Enhanced Trade Analysis System...\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
            "Please update transformers, TRL and unsloth via:\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n",
            "Unsloth 2024.10.7 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trade data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing records:   0%|          | 0/41398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd171ce51c446568dce7b5ffc4da5ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 41398 records\n",
            "Building new semantic index...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating text representations:   0%|          | 0/41398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae690342c366410bb64fe9271fc2bc74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/1294 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0d24007ae9b4681830001a0ec7d461d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n",
            "Saving semantic index...\n",
            "Successfully saved semantic index with 41398 records\n",
            "Successfully loaded all indices!\n",
            "\n",
            "Enhanced Trade Analysis System Ready!\n",
            "Enter your queries about trade data. The system will provide detailed analysis.\n",
            "Type 'quit' to exit.\n",
            "\n",
            "Enter your analysis question: give me the main trends of rice products in q1 2024\n",
            "\n",
            "Parsed Query Components:\n",
            "Time Info: {'year': 2024, 'month': None, 'quarter': 1, 'time_range': (datetime.datetime(2024, 1, 1, 0, 0), datetime.datetime(2024, 3, 31, 0, 0)), 'is_relative': False}\n",
            "Location Info: {'countries': set(), 'regions': set(), 'context': None}\n",
            "Product Info: {'main_category': 'rice', 'subcategory': None, 'variants': set()}\n",
            "Found 15 relevant records after filtering\n",
            "\n",
            "Results Summary:\n",
            "Total Records: 15\n",
            "Total Value: 1,782,931,016.20 PKR\n",
            "Date Range: 2024-01-05 00:00:00 to 2024-03-30 00:00:00\n",
            "\n",
            "Top Origins by Value:\n",
            "- Indonesia: 895,160,000.00 PKR (50.2%)\n",
            "- United kingdom: 336,920,000.00 PKR (18.9%)\n",
            "- Cameroon: 169,921,180.00 PKR (9.5%)\n",
            "- Spain: 145,906,050.00 PKR (8.2%)\n",
            "- Netherlands: 125,346,686.20 PKR (7.0%)\n",
            "\n",
            "Top Products by Value:\n",
            "- Pakistan irri-6 white rice more than 5 pct and 7 pct maximum broken  from 2023/2024 crop year  packing in new single polypropylene bags of 50 kgs nett each, weight of empty = 120 grams (2% empty bags foc): 895,160,000.00 PKR (50.2%)\n",
            "- Brown rice crop (2023-2024)=further detail as per invoice attached: 462,266,686.20 PKR (25.9%)\n",
            "- (sabi) pakistan white rice 25% broken crop year 2023-2024 packed in 50 kg single polypropylene bags: 169,921,180.00 PKR (9.5%)\n",
            "- Brown rice crop (2023-2024) -further detail as per invoice attached: 145,906,050.00 PKR (8.2%)\n",
            "- Pakistani long grainwhite rice irri-6, 5% broken doublepolished & sortexed (freshly milledstock) crop year 2023/2024 packing new single polypropylene bagsweighing 50 kilograms nett (tare weight 140grams): 82,883,500.00 PKR (4.6%)\n",
            "Warning: Could not save chart time_series: 'EnhancedTradeAnalysisSystem' object has no attribute 'chart_store_path'\n",
            "\n",
            "Retrieved Records:\n",
            "================================================================================\n",
            "\n",
            "Record 1:\n",
            "Date: 2024-01-30\n",
            "Product: BROWN RICE CROP (2023-2024)=FURTHER DETAIL AS PER INVOICE ATTACHED\n",
            "Origin: United Kingdom\n",
            "Value: 89,456,000.00 PKR\n",
            "----------------------------------------\n",
            "\n",
            "Record 2:\n",
            "Date: 2024-01-12\n",
            "Product: BROWN RICE CROP (2023-2024)=FURTHER DETAIL AS PER INVOICE ATTACHED\n",
            "Origin: United Kingdom\n",
            "Value: 44,976,000.00 PKR\n",
            "----------------------------------------\n",
            "\n",
            "Record 3:\n",
            "Date: 2024-01-30\n",
            "Product: BROWN RICE CROP (2023-2024)=FURTHER DETAIL AS PER INVOICE ATTACHED\n",
            "Origin: United Kingdom\n",
            "Value: 22,364,000.00 PKR\n",
            "----------------------------------------\n",
            "\n",
            "Record 4:\n",
            "Date: 2024-01-15\n",
            "Product: BROWN RICE CROP (2023-2024)=FURTHER DETAIL AS PER INVOICE ATTACHED\n",
            "Origin: United Kingdom\n",
            "Value: 56,200,000.00 PKR\n",
            "----------------------------------------\n",
            "\n",
            "Record 5:\n",
            "Date: 2024-01-12\n",
            "Product: BROWN RICE CROP (2023-2024)=FURTHER DETAIL AS PER INVOICE ATTACHED\n",
            "Origin: United Kingdom\n",
            "Value: 11,244,000.00 PKR\n",
            "----------------------------------------\n",
            "\n",
            "Summary Statistics:\n",
            "================================================================================\n",
            "Total Records: 15\n",
            "Total Value: 1,782,931,016.20 PKR\n",
            "Date Range: 2024-01-05 00:00:00 to 2024-03-30 00:00:00\n",
            "\n",
            "Top Origins by Value:\n",
            "- Indonesia: 895,160,000.00 PKR (50.2%)\n",
            "- United kingdom: 336,920,000.00 PKR (18.9%)\n",
            "- Cameroon: 169,921,180.00 PKR (9.5%)\n",
            "- Spain: 145,906,050.00 PKR (8.2%)\n",
            "- Netherlands: 125,346,686.20 PKR (7.0%)\n",
            "\n",
            "Top Products by Value:\n",
            "- Pakistan irri-6 white rice more than 5 pct and 7 pct maximum broken  from 2023/2024 crop year  packing in new single polypropylene bags of 50 kgs nett each, weight of empty = 120 grams (2% empty bags foc): 895,160,000.00 PKR (50.2%)\n",
            "- Brown rice crop (2023-2024)=further detail as per invoice attached: 462,266,686.20 PKR (25.9%)\n",
            "- (sabi) pakistan white rice 25% broken crop year 2023-2024 packed in 50 kg single polypropylene bags: 169,921,180.00 PKR (9.5%)\n",
            "- Brown rice crop (2023-2024) -further detail as per invoice attached: 145,906,050.00 PKR (8.2%)\n",
            "- Pakistani long grainwhite rice irri-6, 5% broken doublepolished & sortexed (freshly milledstock) crop year 2023/2024 packing new single polypropylene bagsweighing 50 kilograms nett (tare weight 140grams): 82,883,500.00 PKR (4.6%)\n",
            "\n",
            "AI Analysis:\n",
            "================================================================================\n",
            ": { <{分析} >: **Main Trend Analysis of Rice Products in Q1 4** <{结论} >: \"**Rice Exports to Indonesia leads the sector with approximately $898M, dominant at 51% of total value.**\" The significant share of Indonesian exports indicates an robust market demand for Pakistani rice.\" <{visualization} >: [ {value: 806728029.3}, {value： 402010814.0}, {height: 5}, {category: \" white Rice, Broke 5%\", { category: \" brown Rice\", { height: 3 } }, { category(\"long Grains\"), { height: [ 2 } }}] <{metrics} > - **Total Value:** $1,783,001,013.0 - **Market Share Distribution:** - Indonesia: 49.7%, United Kingdom: 26.1%, Cameroon: 9%, Spain: 7%, Netherlands: 6.6%. - **Growth comparing Q1 '24 to Q1'23:** 15% increase in total export value.\" <{\\ analysis} > \"In Q1, the growth in rice export values from Pakistan by approximately 10% year-on-year suggests a solidify position in global markets. Key factors include increased demand from South Asian countries like India and expanded distribution networks. The strong performance of exported brands like PAKa-6 has been central to this trend.\" <\\end>\n",
            "\n",
            "Visualizations have been saved to: /content/drive2/MyDrive/trade_analysis_model/charts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade unsloth transformers tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-9ckJGqjEEk",
        "outputId": "5b265c32-0682-404a-a8f4-fdd5cc355585"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.10.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: unsloth-zoo in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.10.4)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.0+cu121)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.28.post2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.1)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.1)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.8.14)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.34.2)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.11.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.24.7)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (13.9.3)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "jT3tXRCDQAN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49711ae-1040-49df-9a64-8fe554925c79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive2', force_remount=True)"
      ],
      "metadata": {
        "id": "QQV9PykE9sUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a99c8b3-2976-4e12-caac-64145bc81810"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: drive in /usr/local/lib/python3.10/dist-packages (0.4.4)\n",
            "Requirement already satisfied: google-api-python-client<3.0.0,>=2.64.0 in /usr/local/lib/python3.10/dist-packages (from drive) (2.137.0)\n",
            "Requirement already satisfied: httplib2<0.21.0,>=0.20.4 in /usr/local/lib/python3.10/dist-packages (from drive) (0.20.4)\n",
            "Requirement already satisfied: oauth2client<5.0.0,>=4.1.3 in /usr/local/lib/python3.10/dist-packages (from drive) (4.1.3)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from drive) (3.1.5)\n",
            "Requirement already satisfied: python-magic<0.5.0,>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from drive) (0.4.27)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.64.0->drive) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.64.0->drive) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.64.0->drive) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.64.0->drive) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.21.0,>=0.20.4->drive) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client<5.0.0,>=4.1.3->drive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client<5.0.0,>=4.1.3->drive) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client<5.0.0,>=4.1.3->drive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client<5.0.0,>=4.1.3->drive) (1.16.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl<4.0.0,>=3.0.10->drive) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (1.24.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client<3.0.0,>=2.64.0->drive) (5.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.64.0->drive) (2024.8.30)\n",
            "Mounted at /content/drive2\n"
          ]
        }
      ]
    }
  ]
}